import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import gradio as gr
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pickle
from pathlib import Path
from tqdm import tqdm
import time

class AdvancedPoseRecognition:
    """é«˜çº§äººä½“å§¿æ€è¯†åˆ«ç³»ç»Ÿ"""
    
    def __init__(self, data_path='datasets/Human Action Recognition'):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.data_path = data_path
        self.train_csv = os.path.join(data_path, 'Training_set.csv')
        self.test_csv = os.path.join(data_path, 'Testing_set.csv')
        self.train_image_dir = os.path.join(data_path, 'train')
        self.test_image_dir = os.path.join(data_path, 'test')
        
        self.model = None
        self.label_encoder = None
        self.class_names = []
        
        self.image_size = (224, 224)
        self.batch_size = 32
        self.epochs = 50
        
        print("Advanced Pose Recognition System Initialized")
    
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print("Loading dataset...")
        
        # è¯»å–è®­ç»ƒæ•°æ®
        train_df = pd.read_csv(self.train_csv)
        print(f"Training samples: {len(train_df)}")
        
        # è·å–ç±»åˆ«æ ‡ç­¾
        self.class_names = sorted(train_df['label'].unique())
        print(f"Number of classes: {len(self.class_names)}")
        print(f"Classes: {', '.join(self.class_names)}")
        
        # åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.class_names)
        
        return train_df
    
    def build_model(self):
        """æ„å»ºåŸºäºEfficientNetV2çš„é«˜çº§æ¨¡å‹"""
        print("Building EfficientNetV2 model...")
        
        # ä½¿ç”¨EfficientNetV2-Sä½œä¸ºåŸºç¡€æ¨¡å‹
        base_model = keras.applications.EfficientNetV2S(
            include_top=False,
            weights='imagenet',
            input_shape=(224, 224, 3),
            pooling='avg'
        )
        
        # å†»ç»“åŸºç¡€æ¨¡å‹
        base_model.trainable = False
        
        # æ„å»ºå®Œæ•´æ¨¡å‹
        inputs = keras.Input(shape=(224, 224, 3))
        
        # æ•°æ®å¢å¼º
        x = layers.RandomFlip('horizontal')(inputs)
        x = layers.RandomRotation(0.1)(x)
        x = layers.RandomZoom(0.1)(x)
        x = layers.RandomContrast(0.1)(x)
        
        # é¢„å¤„ç†
        x = keras.applications.efficientnet_v2.preprocess_input(x)
        
        # åŸºç¡€æ¨¡å‹
        x = base_model(x, training=False)
        
        # è‡ªå®šä¹‰åˆ†ç±»å±‚
        x = layers.Dense(512, activation='relu')(x)
        x = layers.Dropout(0.5)(x)
        x = layers.Dense(256, activation='relu')(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(128, activation='relu')(x)
        x = layers.Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚
        outputs = layers.Dense(len(self.class_names), activation='softmax')(x)
        
        # åˆ›å»ºæ¨¡å‹
        model = keras.Model(inputs, outputs, name='AdvancedPoseRecognition')
        
        return model, base_model
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("Starting training...")
        
        # åŠ è½½æ•°æ®
        train_df = self.load_data()
        
        # æ„å»ºæ¨¡å‹
        model, base_model = self.build_model()
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=1e-3),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # å‡†å¤‡æ•°æ®
        images = []
        labels = []
        
        print("Loading images...")
        for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Processing images"):
            img_path = os.path.join(self.train_image_dir, row['filename'])
            
            if os.path.exists(img_path):
                img = cv2.imread(img_path)
                if img is not None:
                    img = cv2.resize(img, self.image_size)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    images.append(img)
                    labels.append(row['label'])
        
        images = np.array(images)
        labels = np.array(self.label_encoder.transform(labels))
        
        print(f"Successfully loaded {len(images)} images")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            images, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        print(f"Training samples: {len(X_train)}")
        print(f"Validation samples: {len(X_val)}")
        
        # å›è°ƒå‡½æ•°
        callbacks = [
            keras.callbacks.ModelCheckpoint(
                'efficientnetv2_best_model.h5',
                save_best_only=True,
                monitor='val_accuracy',
                mode='max'
            ),
            keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_accuracy',
                factor=0.5,
                patience=5,
                min_lr=1e-7
            )
        ]
        
        # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼ˆå†»ç»“åŸºç¡€æ¨¡å‹ï¼‰
        print("\n=== Phase 1: Training with frozen base model ===")
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=20,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼ˆè§£å†»åŸºç¡€æ¨¡å‹ï¼‰
        print("\n=== Phase 2: Fine-tuning with unfrozen base model ===")
        base_model.trainable = True
        
        # é‡æ–°ç¼–è¯‘ï¼ˆä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼‰
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=1e-5),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # ç»§ç»­è®­ç»ƒ
        history_fine = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=30,
            batch_size=self.batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model.save('efficientnetv2_final_model.h5')
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        with open('efficientnetv2_label_encoder.pkl', 'wb') as f:
            pickle.dump(self.label_encoder, f)
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_name': 'EfficientNetV2',
            'num_classes': len(self.class_names),
            'image_size': self.image_size,
            'class_names': self.class_names.tolist(),
            'final_accuracy': history_fine.history['val_accuracy'][-1]
        }
        
        with open('efficientnetv2_model_info.pkl', 'wb') as f:
            pickle.dump(model_info, f)
        
        self.model = model
        print("\nâœ… Training completed!")
        print(f"Final validation accuracy: {history_fine.history['val_accuracy'][-1]:.4f}")
    
    def load_trained_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
        print("Loading trained model...")
        
        try:
            # åŠ è½½æ¨¡å‹
            self.model = keras.models.load_model('efficientnetv2_final_model.h5')
            
            # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
            with open('efficientnetv2_label_encoder.pkl', 'rb') as f:
                self.label_encoder = pickle.load(f)
            
            # åŠ è½½æ¨¡å‹ä¿¡æ¯
            with open('efficientnetv2_model_info.pkl', 'rb') as f:
                model_info = pickle.load(f)
                self.class_names = model_info['class_names']
            
            print("âœ… Model loaded successfully!")
            print(f"Number of classes: {len(self.class_names)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            return False
    
    def predict_image(self, image):
        """é¢„æµ‹å•å¼ å›¾åƒ"""
        if self.model is None:
            if not self.load_trained_model():
                return None, None
        
        # é¢„å¤„ç†
        if isinstance(image, str):
            image = cv2.imread(image)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # è°ƒæ•´å¤§å°
        image_resized = cv2.resize(image, self.image_size)
        image_normalized = image_resized.astype(np.float32) / 255.0
        
        # é¢„æµ‹
        input_data = np.expand_dims(image_normalized, axis=0)
        predictions = self.model.predict(input_data, verbose=0)
        
        # è·å–Topé¢„æµ‹
        top_3_idx = np.argsort(predictions[0])[-3:][::-1]
        
        results = []
        for idx in top_3_idx:
            class_name = self.label_encoder.inverse_transform([idx])[0]
            confidence = float(predictions[0][idx])
            results.append({
                'class': class_name,
                'confidence': confidence
            })
        
        return results[0]['class'], results[0]['confidence'], results
    
    def create_gradio_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if self.model is None:
            if not self.load_trained_model():
                return None
        
        def predict_and_display(image):
            """é¢„æµ‹å¹¶æ˜¾ç¤ºç»“æœ"""
            if image is None:
                return None, "Please upload an image", ""
            
            start_time = time.time()
            top_class, top_confidence, all_results = self.predict_image(image)
            inference_time = time.time() - start_time
            
            # å‡†å¤‡ç»“æœæ–‡æœ¬
            result_text = f"### ğŸ” Prediction Results\n\n"
            result_text += f"**Action:** {top_class}\n\n"
            result_text += f"**Confidence:** {top_confidence*100:.2f}%\n\n"
            result_text += f"**Inference Time:** {inference_time*1000:.1f}ms\n\n"
            result_text += "---\n\n### Top 3 Predictions:\n\n"
            
            for i, result in enumerate(all_results, 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰"
                result_text += f"{emoji} **{result['class']}**: {result['confidence']*100:.2f}%\n"
            
            return image, result_text, f"{top_class} ({top_confidence*100:.1f}%)"
        
        def predict_webcam(image):
            """å®æ—¶æ‘„åƒå¤´é¢„æµ‹"""
            if image is None:
                return image, "Waiting for webcam..."
            
            top_class, top_confidence, _ = self.predict_image(image)
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶ç»“æœ
            image_with_text = image.copy()
            cv2.putText(image_with_text, f"Action: {top_class}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(image_with_text, f"Conf: {top_confidence*100:.1f}%", 
                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            result_text = f"**Action:** {top_class}\n\n**Confidence:** {top_confidence*100:.2f}%"
            
            return image_with_text, result_text
        
        # åˆ›å»ºç•Œé¢
        with gr.Blocks(theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="sky",
        )) as demo:
            gr.Markdown(
                """
                # ğŸ¤¸ Advanced Human Pose Recognition System
                
                ## Built with EfficientNetV2 + Gradio
                
                This system uses state-of-the-art deep learning to recognize 15 different human actions with high accuracy.
                
                ---
                """
            )
            
            with gr.Tabs():
                # å›¾åƒè¯†åˆ«æ ‡ç­¾é¡µ
                with gr.Tab("ğŸ“· Image Recognition"):
                    with gr.Row():
                        with gr.Column():
                            image_input = gr.Image(label="Upload Image", type="numpy")
                            predict_btn = gr.Button("ğŸ”® Predict", variant="primary", size="lg")
                        
                        with gr.Column():
                            output_image = gr.Image(label="Result")
                            output_text = gr.Markdown(label="Prediction Results")
                            output_label = gr.Textbox(label="Quick Result")
                    
                    gr.Examples(
                        examples=[
                            [os.path.join(self.train_image_dir, f) for f in os.listdir(self.train_image_dir)[:4]]
                        ],
                        inputs=image_input
                    )
                    
                    predict_btn.click(
                        predict_and_display,
                        inputs=[image_input],
                        outputs=[output_image, output_text, output_label]
                    )
                
                # å®æ—¶æ‘„åƒå¤´æ ‡ç­¾é¡µ
                with gr.Tab("ğŸ“¹ Real-time Webcam"):
                    with gr.Row():
                        webcam_input = gr.Image(label="Webcam Feed", source="webcam", streaming=True)
                        webcam_output = gr.Image(label="Real-time Prediction")
                    
                    with gr.Row():
                        webcam_result = gr.Markdown(label="Detection Result")
                    
                    webcam_input.change(
                        predict_webcam,
                        inputs=[webcam_input],
                        outputs=[webcam_output, webcam_result]
                    )
            
            gr.Markdown(
                """
                ---
                
                ### ğŸ“Š Supported Actions (15 Classes)
                
                | Action | Action | Action | Action | Action |
                |--------|--------|--------|--------|--------|
                | sitting | using_laptop | hugging | sleeping | drinking |
                | clapping | dancing | cycling | calling | laughing |
                | eating | fighting | listening_to_music |  |  |
                
                ---
                
                ### ğŸ’¡ Tips for Better Recognition
                
                - Ensure the person is clearly visible in the image
                - Good lighting conditions improve accuracy
                - The action should be clearly performed
                - Front or side view works best
                """
            )
        
        return demo
    
    def run(self):
        """è¿è¡ŒGradioåº”ç”¨"""
        demo = self.create_gradio_interface()
        if demo:
            demo.launch(share=True, server_name="0.0.0.0", server_port=7860)
        else:
            print("Failed to create interface")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Advanced Pose Recognition System')
    parser.add_argument('--train', action='store_true', help='Train the model')
    parser.add_argument('--run', action='store_true', help='Run the Gradio interface')
    parser.add_argument('--data_path', default='datasets/Human Action Recognition', help='Path to dataset')
    
    args = parser.parse_args()
    
    system = AdvancedPoseRecognition(data_path=args.data_path)
    
    if args.train:
        system.train()
    elif args.run:
        system.run()
    else:
        print("Please specify --train or --run")
        print("Example: python advanced_pose_recognition.py --run")

if __name__ == "__main__":
    main()
